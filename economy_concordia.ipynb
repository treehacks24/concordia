{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE-6f595AybO"
      },
      "source": [
        "```\n",
        "Copyright 2023 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWgEkOAO9OVz"
      },
      "source": [
        "# An example illustrating how to use the inventory component.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2TwJrZ08wXz"
      },
      "source": [
        "## Init and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibSOXvFOEs7H"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-deepmind/concordia.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cd .. && cd concordia && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-qLG5ExLqpWa"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "\n",
        "import collections\n",
        "import concurrent.futures\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#from google.colab import widgets  # pytype: disable=import-error\n",
        "from IPython import display\n",
        "\n",
        "from concordia.agents import basic_agent\n",
        "from concordia.components import agent as components\n",
        "from concordia import components as generic_components\n",
        "from concordia.associative_memory import associative_memory\n",
        "from concordia.associative_memory import blank_memories\n",
        "from concordia.associative_memory import embedder_st5\n",
        "from concordia.associative_memory import formative_memories\n",
        "from concordia.associative_memory import importance_function\n",
        "from concordia.clocks import game_clock\n",
        "from concordia.components import game_master as gm_components\n",
        "from concordia.environment import game_master\n",
        "from concordia.metrics import goal_achievement\n",
        "from concordia.metrics import common_sense_morality\n",
        "from concordia.metrics import opinion_of_others\n",
        "from concordia.language_model import gpt_model\n",
        "from concordia.language_model import gcloud_model\n",
        "from concordia.utils import html as html_lib\n",
        "from concordia.utils import measurements as measurements_lib\n",
        "from concordia.utils import plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I3OtW8flCJSC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Importing a function (__inference_<lambda>_9720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_<lambda>_3354) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_<lambda>_6722) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        }
      ],
      "source": [
        "# Setup sentence encoder\n",
        "embedder = embedder_st5.EmbedderST5()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from collections.abc import Collection, Sequence\n",
        "from concordia.language_model import language_model\n",
        "from concordia.utils import measurements as measurements_lib\n",
        "import openai\n",
        "from typing_extensions import override\n",
        "import os\n",
        "\n",
        "_MAX_MULTIPLE_CHOICE_ATTEMPTS = 3\n",
        "\n",
        "\n",
        "class GptLanguageModel(language_model.LanguageModel):\n",
        "  \"\"\"Language Model that uses OpenAI GPT models.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      api_key: str,\n",
        "      model_name: str,\n",
        "      measurements: measurements_lib.Measurements | None = None,\n",
        "      channel: str = language_model.DEFAULT_STATS_CHANNEL,\n",
        "  ):\n",
        "    \"\"\"Initializes the instance.\n",
        "\n",
        "    Args:\n",
        "      api_key: The API key to use when accessing the OpenAI API.\n",
        "      model_name: The language model to use. For more details, see\n",
        "        https://platform.openai.com/docs/guides/text-generation/which-model-should-i-use.\n",
        "      measurements: The measurements object to log usage statistics to.\n",
        "      channel: The channel to write the statistics to.\n",
        "    \"\"\"\n",
        "    self._api_key = api_key\n",
        "    self._model_name = model_name\n",
        "    self._measurements = measurements\n",
        "    self._channel = channel\n",
        "    self._client = openai.OpenAI(\n",
        "        api_key=api_key, base_url='https://api.together.xyz/v1')\n",
        "\n",
        "  @override\n",
        "  def sample_text(\n",
        "      self,\n",
        "      prompt: str,\n",
        "      *,\n",
        "      max_tokens: int = language_model.DEFAULT_MAX_TOKENS,\n",
        "      max_characters: int = language_model.DEFAULT_MAX_CHARACTERS,\n",
        "      terminators: Collection[str] = language_model.DEFAULT_TERMINATORS,\n",
        "      temperature: float = language_model.DEFAULT_TEMPERATURE,\n",
        "      timeout: float = language_model.DEFAULT_TIMEOUT_SECONDS,\n",
        "      seed: int | None = None,\n",
        "  ) -> str:\n",
        "    messages = [{'role': 'user', 'content': prompt}]\n",
        "\n",
        "    response = self._client.chat.completions.create(\n",
        "        model=self._model_name,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        timeout=timeout,\n",
        "        stop=terminators,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    if self._measurements is not None:\n",
        "      self._measurements.publish_datum(\n",
        "          self._channel,\n",
        "          {'raw_text_length': len(response.choices[0].message.content)},\n",
        "      )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  @override\n",
        "  def sample_choice(\n",
        "      self,\n",
        "      prompt: str,\n",
        "      responses: Sequence[str],\n",
        "      *,\n",
        "      seed: int | None = None,\n",
        "  ) -> tuple[int, str, dict[str, float]]:\n",
        "    max_characters = len(max(responses, key=len))\n",
        "\n",
        "    attempts = 1\n",
        "    prompt = (\n",
        "        prompt\n",
        "        + '\\nRespond EXACTLY with one of the following strings:\\n'\n",
        "        + '\\n'.join(responses) + '.'\n",
        "    )\n",
        "\n",
        "    for _ in range(_MAX_MULTIPLE_CHOICE_ATTEMPTS):\n",
        "      # Increase temperature after the first failed attempt.\n",
        "      temperature = 0.0\n",
        "      if attempts > 1:\n",
        "        temperature = 0.5\n",
        "\n",
        "      sample = self.sample_text(\n",
        "          prompt,\n",
        "          max_characters=max_characters,\n",
        "          temperature=temperature,\n",
        "          seed=seed,\n",
        "      )\n",
        "      try:\n",
        "        idx = responses.index(sample)\n",
        "      except ValueError:\n",
        "        attempts += 1\n",
        "        continue\n",
        "      else:\n",
        "        if self._measurements is not None:\n",
        "          self._measurements.publish_datum(\n",
        "              self._channel, {'choices_calls': attempts}\n",
        "          )\n",
        "        debug = {}\n",
        "        return idx, responses[idx], debug\n",
        "\n",
        "    raise language_model.InvalidResponseError(\n",
        "        'Too many multiple choice attempts.'\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cugwvFIKv5AS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Importing a function (__inference_<lambda>_9720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_<lambda>_3354) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
            "WARNING:absl:Importing a function (__inference_<lambda>_6722) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
          ]
        }
      ],
      "source": [
        "# # @title Language Model - pick your model and provide keys\n",
        "# CLOUD_PROJECT_ID = '' #@param {type: 'string'}\n",
        "# GPT_API_KEY = '4834f3e70ce6aa377ab7df060edda036204f2f653b6958a57e08ef0a307c894b' #@param {type: 'string'}\n",
        "# GPT_MODEL_NAME = 'Qwen/Qwen1.5-72B-Chat' #@param {type: 'string'}\n",
        "\n",
        "# USE_CLOUD = False #@param {type: 'boolean'}\n",
        "\n",
        "# if USE_CLOUD:\n",
        "#   model = gcloud_model.CloudLanguageModel(project_id= CLOUD_PROJECT_ID)\n",
        "# else:\n",
        "#   model = GptLanguageModel(api_key=GPT_API_KEY, model_name=GPT_MODEL_NAME)\n",
        "\n",
        "# #model.test\n",
        "\n",
        "\n",
        "#@title Setup sentence encoder\n",
        "embedder = embedder_st5.EmbedderST5()\n",
        "\n",
        "\n",
        "# @title Language Model - pick your model and provide keys\n",
        "CLOUD_PROJECT_ID = '' #@param {type: 'string'}\n",
        "GPT_API_KEY = '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' #@param {type: 'string'}\n",
        "GPT_MODEL_NAME = 'gpt-4-1106-preview' #@param {type: 'string'}\n",
        "\n",
        "USE_CLOUD = False #@param {type: 'boolean'}\n",
        "\n",
        "if USE_CLOUD:\n",
        "  model = gcloud_model.CloudLanguageModel(project_id= CLOUD_PROJECT_ID)\n",
        "else:\n",
        "  model = gpt_model.GptLanguageModel(api_key=GPT_API_KEY, model_name=GPT_MODEL_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9HYjZgyakc_"
      },
      "source": [
        "## Configuring the generic knowledge of players and GM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TeVYseoD2WYa"
      },
      "outputs": [],
      "source": [
        "#@title Make the clock\n",
        "TIME_STEP = datetime.timedelta(minutes=20)\n",
        "SETUP_TIME = datetime.datetime(hour=20, year=2024, month=10, day=1)\n",
        "\n",
        "START_TIME = datetime.datetime(hour=12, year=2024, month=10, day=2)\n",
        "clock = game_clock.MultiIntervalClock(\n",
        "    start=SETUP_TIME,\n",
        "    step_sizes=[TIME_STEP, datetime.timedelta(seconds=10)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "otZ6Xg6BnohY"
      },
      "outputs": [],
      "source": [
        "#@title Importance models\n",
        "importance_model = importance_function.AgentImportanceModel(model)\n",
        "importance_model_gm = importance_function.ConstantImportanceModel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "b8vWoQ6by51N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In the small town of Riverbend, Alice is employed at The Oddments and Oddities Emporium, a general store that specializes in selling magic beans, typically priced at $10 each.\n"
          ]
        }
      ],
      "source": [
        "# @title Generic memories are memories that all players and GM share.\n",
        "\n",
        "shared_memories = [\n",
        "    'Riverbend is a small town.',\n",
        "    ('There is a general store in Riverbend called The Oddments and Oddities ' +\n",
        "     'Emporium.'),\n",
        "    'Alice works in the general store.',\n",
        "    'The Oddments and Oddities Emporium sells magic beans.',\n",
        "    'The usual price of a magic bean is $10 per bean.',\n",
        "]\n",
        "\n",
        "# The generic context will be used for the NPC context. It reflects general\n",
        "# knowledge and is possessed by all characters.\n",
        "shared_context = model.sample_text(\n",
        "    'Summarize the following passage in a concise and insightful fashion:\\n'\n",
        "    + '\\n'.join(shared_memories)\n",
        "    + '\\n'\n",
        "    + 'Summary:'\n",
        ")\n",
        "print(shared_context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGwXtHiGn8H3"
      },
      "source": [
        "## Configure and build the players\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QHJ25gnUn0y6"
      },
      "outputs": [],
      "source": [
        "blank_memory_factory = blank_memories.MemoryFactory(\n",
        "    model=model,\n",
        "    embedder=embedder,\n",
        "    importance=importance_model.importance,\n",
        "    clock_now=clock.now,\n",
        ")\n",
        "\n",
        "formative_memory_factory = formative_memories.FormativeMemoryFactory(\n",
        "    model=model,\n",
        "    shared_memories=shared_memories,\n",
        "    blank_memory_factory_call=blank_memory_factory.make_blank_memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBCXUQ8sayzj"
      },
      "source": [
        "## Functions to build the players"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NkgQEQ4JoDtZ"
      },
      "outputs": [],
      "source": [
        "def build_agent(agent_config,\n",
        "                player_names: list[str],\n",
        "                measurements: measurements_lib.Measurements | None = None):\n",
        "\n",
        "  mem = formative_memory_factory.make_memories(agent_config)\n",
        "\n",
        "  time = generic_components.report_function.ReportFunction(\n",
        "    name='Current time',\n",
        "    function=clock.current_time_interval_str,\n",
        "  )\n",
        "\n",
        "  current_obs = components.observation.Observation(\n",
        "            agent_name=agent_config.name,\n",
        "      clock_now=clock.now,\n",
        "      memory=mem,\n",
        "      timeframe=clock.get_step_size(),\n",
        "      component_name='current observations',\n",
        "  )\n",
        "\n",
        "  self_perception = components.self_perception.SelfPerception(\n",
        "      name=f'What kind of person is {agent_config.name}? ',\n",
        "      model=model,\n",
        "      memory=mem,\n",
        "      agent_name=agent_config.name,\n",
        "      clock_now=clock.now,\n",
        "      verbose=True,\n",
        "  )\n",
        "  situation_perception = components.situation_perception.SituationPerception(\n",
        "      name=f'What kind of situation is {agent_config.name} in right now? ',\n",
        "      model=model,\n",
        "      memory=mem,\n",
        "      agent_name=agent_config.name,\n",
        "      components=[current_obs],\n",
        "      clock_now=clock.now,\n",
        "      verbose=True,\n",
        "  )\n",
        "  person_by_situation = components.person_by_situation.PersonBySituation(\n",
        "      name=(f'What would a person like {agent_config.name} do in a situation ' +\n",
        "            'like this? '),\n",
        "      model=model,\n",
        "      memory=mem,\n",
        "      agent_name=agent_config.name,\n",
        "      clock_now=clock.now,\n",
        "      components=[self_perception, situation_perception],\n",
        "  )\n",
        "\n",
        "  initial_goal_component = generic_components.constant.ConstantComponent(\n",
        "      state=agent_config.goal)\n",
        "  plan = components.plan.SimPlan(\n",
        "      model,\n",
        "      mem,\n",
        "      agent_config.name,\n",
        "      components=[initial_goal_component, person_by_situation],\n",
        "      goal=person_by_situation,\n",
        "      timescale='the next hour',\n",
        "      time_adverb='detailed',\n",
        "      verbose=False,\n",
        "  )\n",
        "\n",
        "  persona = generic_components.sequential.Sequential(\n",
        "      name='persona',\n",
        "      components=[\n",
        "          self_perception,\n",
        "          situation_perception,\n",
        "          person_by_situation,\n",
        "          plan,\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  summary_obs = components.observation.ObservationSummary(\n",
        "      agent_name=agent_config.name,\n",
        "      model=model,\n",
        "      clock_now=clock.now,\n",
        "      memory=mem,\n",
        "      components=[persona, current_obs],\n",
        "      timeframe_delta_from=datetime.timedelta(hours=4),\n",
        "      timeframe_delta_until=datetime.timedelta(hours=1),\n",
        "      component_name='summary of observations',\n",
        "  )\n",
        "\n",
        "  goal_metric = goal_achievement.GoalAchievementMetric(\n",
        "      model=model,\n",
        "      player_name=agent_config.name,\n",
        "      player_goal=agent_config.goal,\n",
        "      clock=clock,\n",
        "      name='Goal Achievement',\n",
        "      measurements=measurements,\n",
        "      channel='goal_achievement',\n",
        "      verbose=False,\n",
        "  )\n",
        "  morality_metric = common_sense_morality.CommonSenseMoralityMetric(\n",
        "      model=model,\n",
        "      player_name=agent_config.name,\n",
        "      clock=clock,\n",
        "      name='Morality',\n",
        "      verbose=False,\n",
        "      measurements=measurements,\n",
        "      channel='common_sense_morality',\n",
        "  )\n",
        "  agent = basic_agent.BasicAgent(\n",
        "      model,\n",
        "      mem,\n",
        "      agent_name=agent_config.name,\n",
        "      clock=clock,\n",
        "      verbose=False,\n",
        "      components=[persona,\n",
        "                  summary_obs,\n",
        "                  current_obs,\n",
        "                  time,\n",
        "                  goal_metric,\n",
        "                  morality_metric],\n",
        "      update_interval = TIME_STEP\n",
        "  )\n",
        "  reputation_metric = opinion_of_others.OpinionOfOthersMetric(\n",
        "      model=model,\n",
        "      player_name=agent_config.name,\n",
        "      player_names=player_names,\n",
        "      context_fn=agent.state,\n",
        "      clock=clock,\n",
        "      name='Opinion',\n",
        "      verbose=False,\n",
        "      measurements=measurements,\n",
        "      channel='opinion_of_others',\n",
        "      question='What is {opining_player}\\'s opinion of {of_player}?',\n",
        "  )\n",
        "  agent.add_component(reputation_metric)\n",
        "  return agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt8CK2mMbD7q"
      },
      "source": [
        "## Configure and build the players"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YxexdZ6XoR2K"
      },
      "outputs": [],
      "source": [
        "#@title Creating character backgrounds, goals and traits. Modify to explore how it influences the outcomes\n",
        "NUM_PLAYERS = 4\n",
        "\n",
        "player_configs = [\n",
        "    formative_memories.AgentConfig(\n",
        "        name='Alice',\n",
        "        gender='female',\n",
        "        goal='Alice wants to sell as many magic beans as possible.',\n",
        "        context='Alice is passionate about selling magic beans.',\n",
        "        traits='responsibility: high; aggression: low',\n",
        "        extras={'initial_endowment': {'money': 20.0, 'magic beans': 100.0},},\n",
        "    ),\n",
        "    formative_memories.AgentConfig(\n",
        "        name='Bob',\n",
        "        gender='male',\n",
        "        goal='Bob wants to buy lots of magic beans to feed a magic cat.',\n",
        "        context='Bob has a magic cat who loves to eat magic beans.',\n",
        "        traits='responsibility: high; aggression: low',\n",
        "        extras={'initial_endowment': {'money': 75.0, 'magic beans': 0.0},},\n",
        "    ),\n",
        "    formative_memories.AgentConfig(\n",
        "        name='Charlie',\n",
        "        gender='male',\n",
        "        goal=(\n",
        "            'Charlie wants to steal magic beans and then open his own shop ' +\n",
        "            'selling them more cheaply than the Oddments and Oddities ' +\n",
        "            'Emporium. By doing that, Charlie hopes to win over their ' +\n",
        "            'customers by offering to sell the same magic beans at a lower ' +\n",
        "            'price.'\n",
        "        ),\n",
        "        context='Charlie is very stealthy and good at sneaking around',\n",
        "        traits='responsibility: low; aggression: high',\n",
        "        extras={'initial_endowment': {'money': 5.0, 'magic beans': 0.0},},\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b0vQg5tVqlcs"
      },
      "outputs": [
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Request was rejected due to rate limiting. As a free user, your QPS is 1. If you want more, please upgrade your account.', 'type': 'credit_limit', 'param': None, 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m players \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mNUM_PLAYERS) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m----> 8\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mmap(build_agent,\n\u001b[1;32m      9\u001b[0m                         player_configs[:NUM_PLAYERS],\n\u001b[1;32m     10\u001b[0m                         \u001b[38;5;66;03m# All players get the same `player_names`.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                         [player_names] \u001b[38;5;241m*\u001b[39m NUM_PLAYERS,\n\u001b[1;32m     12\u001b[0m                         \u001b[38;5;66;03m# All players get the same `measurements` object.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m                         [measurements] \u001b[38;5;241m*\u001b[39m NUM_PLAYERS):\n\u001b[1;32m     14\u001b[0m     players\u001b[38;5;241m.\u001b[39mappend(agent)\n",
            "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
            "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
            "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
            "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "Cell \u001b[0;32mIn[31], line 5\u001b[0m, in \u001b[0;36mbuild_agent\u001b[0;34m(agent_config, player_names, measurements)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_agent\u001b[39m(agent_config,\n\u001b[1;32m      2\u001b[0m                 player_names: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m      3\u001b[0m                 measurements: measurements_lib\u001b[38;5;241m.\u001b[39mMeasurements \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 5\u001b[0m   mem \u001b[38;5;241m=\u001b[39m \u001b[43mformative_memory_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_memories\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m   time \u001b[38;5;241m=\u001b[39m generic_components\u001b[38;5;241m.\u001b[39mreport_function\u001b[38;5;241m.\u001b[39mReportFunction(\n\u001b[1;32m      8\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent time\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     function\u001b[38;5;241m=\u001b[39mclock\u001b[38;5;241m.\u001b[39mcurrent_time_interval_str,\n\u001b[1;32m     10\u001b[0m   )\n\u001b[1;32m     12\u001b[0m   current_obs \u001b[38;5;241m=\u001b[39m components\u001b[38;5;241m.\u001b[39mobservation\u001b[38;5;241m.\u001b[39mObservation(\n\u001b[1;32m     13\u001b[0m             agent_name\u001b[38;5;241m=\u001b[39magent_config\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     14\u001b[0m       clock_now\u001b[38;5;241m=\u001b[39mclock\u001b[38;5;241m.\u001b[39mnow,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m       component_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent observations\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m   )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concordia/associative_memory/formative_memories.py:149\u001b[0m, in \u001b[0;36mFormativeMemoryFactory.make_memories\u001b[0;34m(self, agent_config)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# All players share generic memories.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shared_memories:\n\u001b[0;32m--> 149\u001b[0m   \u001b[43mmem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m context \u001b[38;5;241m=\u001b[39m agent_config\u001b[38;5;241m.\u001b[39mcontext\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent_config\u001b[38;5;241m.\u001b[39mgoal:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concordia/associative_memory/associative_memory.py:78\u001b[0m, in \u001b[0;36mAssociativeMemory.add\u001b[0;34m(self, text, timestamp, tags, importance)\u001b[0m\n\u001b[1;32m     76\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedder(text)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m   importance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m   timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_now()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concordia/associative_memory/importance_function.py:76\u001b[0m, in \u001b[0;36mAgentImportanceModel.importance\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m     74\u001b[0m zero, \u001b[38;5;241m*\u001b[39m_, one \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_importance_scale\n\u001b[1;32m     75\u001b[0m prompt \u001b[38;5;241m=\u001b[39m interactive_document\u001b[38;5;241m.\u001b[39mInteractiveDocument(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model)\n\u001b[0;32m---> 76\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiple_choice_question\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOn the scale of \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzero\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m to\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mone\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, where \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzero\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m is\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m purely mundane (e.g., brushing teeth, making bed) and\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mone\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m is extremely poignant (e.g., a break\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m up, college acceptance), rate the likely poignancy of the following\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m piece of memory.\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMemory:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mRating: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_importance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_importance_scale) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/concordia/document/interactive_document.py:198\u001b[0m, in \u001b[0;36mInteractiveDocument.multiple_choice_question\u001b[0;34m(self, question, answers)\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_question(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer: (\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m idx, response, debug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_choice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_response(response)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[8], line 91\u001b[0m, in \u001b[0;36mGptLanguageModel.sample_choice\u001b[0;34m(self, prompt, responses, seed)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attempts \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     89\u001b[0m   temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 91\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m   idx \u001b[38;5;241m=\u001b[39m responses\u001b[38;5;241m.\u001b[39mindex(sample)\n",
            "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mGptLanguageModel.sample_text\u001b[0;34m(self, prompt, max_tokens, max_characters, terminators, temperature, timeout, seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_text\u001b[39m(\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m     seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     49\u001b[0m   messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt}]\n\u001b[0;32m---> 51\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m      \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_measurements \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_measurements\u001b[38;5;241m.\u001b[39mpublish_datum(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel,\n\u001b[1;32m     64\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_text_length\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)},\n\u001b[1;32m     65\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/resources/chat/completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:965\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    964\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    988\u001b[0m )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request was rejected due to rate limiting. As a free user, your QPS is 1. If you want more, please upgrade your account.', 'type': 'credit_limit', 'param': None, 'code': None}}"
          ]
        }
      ],
      "source": [
        "player_configs = player_configs[:NUM_PLAYERS]\n",
        "player_names = [player.name for player in player_configs][:NUM_PLAYERS]\n",
        "measurements = measurements_lib.Measurements()\n",
        "\n",
        "players = []\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_PLAYERS) as pool:\n",
        "  for agent in pool.map(build_agent,\n",
        "                        player_configs[:NUM_PLAYERS],\n",
        "                        # All players get the same `player_names`.\n",
        "                        [player_names] * NUM_PLAYERS,\n",
        "                        # All players get the same `measurements` object.\n",
        "                        [measurements] * NUM_PLAYERS):\n",
        "    players.append(agent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vt8ggYUrW8M"
      },
      "source": [
        "## Build GM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W65kHOKQwrv"
      },
      "outputs": [],
      "source": [
        "game_master_memory = associative_memory.AssociativeMemory(\n",
        "   sentence_embedder=embedder,\n",
        "   importance=importance_model_gm.importance,\n",
        "   clock=clock.now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cxivChc633z"
      },
      "outputs": [],
      "source": [
        "# @title Create components of the Game Master\n",
        "player_names = [player.name for player in players]\n",
        "\n",
        "scenario_knowledge = generic_components.constant.ConstantComponent(\n",
        "    state=' '.join(shared_memories),\n",
        "    name='Background')\n",
        "time_display=generic_components.report_function.ReportFunction(\n",
        "    name='Current time',\n",
        "    function=clock.current_time_interval_str,\n",
        "  )\n",
        "important_facts = [\n",
        "    'Magic beans are marked \"magical\" but have no actual magical powers.',\n",
        "    'Magic is not real.',\n",
        "]\n",
        "important_facts_construct = generic_components.constant.ConstantComponent(\n",
        "    state=' '.join(important_facts),\n",
        "    name='Important facts')\n",
        "\n",
        "ItemTypeConfig = gm_components.inventory.ItemTypeConfig\n",
        "money_config = ItemTypeConfig(name='money')\n",
        "magic_beans_config = ItemTypeConfig(\n",
        "    name='magic beans', minimum=0, maximum=np.inf, force_integer=True)\n",
        "player_initial_endowments = {\n",
        "    config.name: config.extras['initial_endowment'] for config in player_configs}\n",
        "inventories = gm_components.inventory.Inventory(\n",
        "    model=model,\n",
        "    memory=game_master_memory,\n",
        "    item_type_configs=[money_config, magic_beans_config],\n",
        "    player_initial_endowments=player_initial_endowments,\n",
        "    clock_now=clock.now,\n",
        "    financial=True,\n",
        "    name='Property',\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "player_status = gm_components.player_status.PlayerStatus(\n",
        "    clock_now=clock.now,\n",
        "    model=model,\n",
        "    memory=game_master_memory,\n",
        "    player_names=player_names)\n",
        "\n",
        "convo_externality = gm_components.conversation.Conversation(\n",
        "    players=players,\n",
        "    model=model,\n",
        "    memory=game_master_memory,\n",
        "    clock=clock,\n",
        "    burner_memory_factory=blank_memory_factory,\n",
        "    components=[player_status, inventories],\n",
        "    cap_nonplayer_characters=3,\n",
        "    shared_context=shared_context,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "direct_effect_externality = gm_components.direct_effect.DirectEffect(\n",
        "    players=players,\n",
        "    model=model,\n",
        "    memory=game_master_memory,\n",
        "    clock_now=clock.now,\n",
        "    verbose=False,\n",
        "    components=[player_status, inventories]\n",
        ")\n",
        "\n",
        "relevant_events = gm_components.relevant_events.RelevantEvents(\n",
        "    clock.now, model, game_master_memory)\n",
        "time_display = gm_components.time_display.TimeDisplay(clock)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_R2BVNOsAwa"
      },
      "outputs": [],
      "source": [
        "# @title Create the game master object\n",
        "env = game_master.GameMaster(\n",
        "    model=model,\n",
        "    memory=game_master_memory,\n",
        "    clock=clock,\n",
        "    players=players,\n",
        "    components=[\n",
        "        scenario_knowledge,\n",
        "        important_facts_construct,\n",
        "        player_status,\n",
        "        convo_externality,\n",
        "        direct_effect_externality,\n",
        "        inventories,\n",
        "        relevant_events,\n",
        "        time_display,\n",
        "    ],\n",
        "    randomise_initiative=True,\n",
        "    player_observes_event=False,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXykV_TdwfKq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2u0bQ1MSCGd"
      },
      "source": [
        "## The RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdTRDaxEZZnN"
      },
      "outputs": [],
      "source": [
        "clock.set(START_TIME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IggLF1aH_hF"
      },
      "outputs": [],
      "source": [
        "for player in players:\n",
        "  game_master_memory.add(\n",
        "      f'{player.name} is at The Oddments and Oddities Emporium.')\n",
        "\n",
        "scenario_premise = (\n",
        "    'Alice, Bob, and Charlie are at The Oddments and Oddities '\n",
        "    'Emporium.'\n",
        ")\n",
        "game_master_memory.add(scenario_premise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIFEzzs5pAJW"
      },
      "outputs": [],
      "source": [
        "for player in players:\n",
        "  player.observe(scenario_premise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Bt87stq76gF"
      },
      "outputs": [],
      "source": [
        "# @title Expect about 2-3 minutes per step.\n",
        "episode_length = 10  # @param {type: 'integer'}\n",
        "for _ in range(episode_length):\n",
        "  env.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnwvpvQ4bnFs"
      },
      "source": [
        "## Summary and analysis of the episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U5FDXvs4HSr"
      },
      "outputs": [],
      "source": [
        "# @title Metrics plotting\n",
        "group_by = collections.defaultdict(lambda: 'player')\n",
        "group_by['opinion_of_others'] = 'of_player'\n",
        "\n",
        "tb = widgets.TabBar([channel for channel in measurements.available_channels()])\n",
        "for channel in measurements.available_channels():\n",
        "  with tb.output_to(channel):\n",
        "    plotting.plot_line_measurement_channel(measurements, channel,\n",
        "                                           group_by=group_by[channel],\n",
        "                                           xaxis='time_str')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j71OiuPot5UV"
      },
      "source": [
        "## Save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4jp0xGXvOAJ"
      },
      "outputs": [],
      "source": [
        "# @title Summarize the entire story.\n",
        "all_gm_memories = env._memory.retrieve_recent(k=10000, add_time=True)\n",
        "\n",
        "detailed_story = '\\n'.join(all_gm_memories)\n",
        "print('len(detailed_story): ', len(detailed_story))\n",
        "# print(detailed_story)\n",
        "\n",
        "episode_summary = model.sample_text(\n",
        "    f'Sequence of events:\\n{detailed_story}'+\n",
        "    '\\nNarratively summarize the above temporally ordered ' +\n",
        "    'sequence of events. Write it as a news report. Summary:\\n',\n",
        "    max_characters=3500, max_tokens=3500, terminators=())\n",
        "print(episode_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALG987t-6j-V"
      },
      "outputs": [],
      "source": [
        "# @title Summarise the perspective of each player\n",
        "player_logs = []\n",
        "player_log_names = []\n",
        "for player in players:\n",
        "  name = player.name\n",
        "  detailed_story = '\\n'.join(player._memory.retrieve_recent(k=1000,\n",
        "                                                            add_time=True))\n",
        "  summary = ''\n",
        "  summary = model.sample_text(\n",
        "      f'Sequence of events that happened to {name}:\\n{detailed_story}'\n",
        "      '\\nWrite a short story that summarises these events.\\n'\n",
        "      ,\n",
        "      max_characters=3500, max_tokens=3500, terminators=())\n",
        "\n",
        "  all_player_mem = player._memory.retrieve_recent(k=1000, add_time=True)\n",
        "  all_player_mem = ['Summary:', summary, 'Memories:'] + all_player_mem\n",
        "  player_html = html_lib.PythonObjectToHTMLConverter(all_player_mem).convert()\n",
        "  player_logs.append(player_html)\n",
        "  player_log_names.append(f'{name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmPOvjVxddye"
      },
      "source": [
        "#Build and display HTML log of the experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyEoGgI05xI0"
      },
      "outputs": [],
      "source": [
        "history_sources = [env, direct_effect_externality, convo_externality]\n",
        "histories_html = [\n",
        "    html_lib.PythonObjectToHTMLConverter(history.get_history()).convert()\n",
        "    for history in history_sources]\n",
        "histories_names = [history.name() for history in history_sources]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNJuo4Dwt5Ui"
      },
      "outputs": [],
      "source": [
        "gm_mem_html = html_lib.PythonObjectToHTMLConverter(all_gm_memories).convert()\n",
        "\n",
        "tabbed_html = html_lib.combine_html_pages(\n",
        "    histories_html + [gm_mem_html] + player_logs,\n",
        "    histories_names + ['GM'] + player_log_names,\n",
        "    summary=episode_summary,\n",
        "    title='Magic beans experiment',\n",
        ")\n",
        "\n",
        "tabbed_html = html_lib.finalise_html(tabbed_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pumxvmrzANOq"
      },
      "outputs": [],
      "source": [
        "display.HTML(tabbed_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX-M9Im_dneG"
      },
      "source": [
        "#Interact with a specific player"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESJ1l7_Kt5Uj"
      },
      "outputs": [],
      "source": [
        "sim_to_interact = 'Alice'  # @param ['Alice', 'Bob','Charlie', 'Dorothy', 'Ellen'] {type:\"string\"}\n",
        "user_identity = 'a close friend'  # @param {type:\"string\"}\n",
        "interaction_premise = f'{sim_to_interact} is talking to {user_identity}\\n'  # @param {type:\"string\"}\n",
        "\n",
        "player_names = [player.name for player in players]\n",
        "player_by_name = {player.name: player for player in players}\n",
        "selected_player = player_by_name[sim_to_interact]\n",
        "interrogation = interaction_premise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q1cYflLt5Uj"
      },
      "outputs": [],
      "source": [
        "utterence_from_user = 'Did you manage to sell any magic beans?'  # @param {type:\"string\"}\n",
        "\n",
        "interrogation += f'{user_identity}: {utterence_from_user}'\n",
        "player_says = selected_player.say(interrogation)\n",
        "interrogation += f'\\n{sim_to_interact}: {player_says}\\n'\n",
        "print(interrogation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D43NMru3qjQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
